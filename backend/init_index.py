import pandas as pd
import os
from services.vector_db import init_chroma

def load_csv(file_name):
    """Charge un fichier CSV"""
    if os.path.exists(file_name):
        df = pd.read_csv(file_name, encoding='utf-8-sig')
        return df
    else:
        raise FileNotFoundError(f"{file_name} introuvable.")

def preprocess_df(df):
    """Pr√©traite le DataFrame"""
    # Nettoyer les noms de colonnes
    df.columns = [col.strip().lower() for col in df.columns]
    
    # Mapping des colonnes
    col_mapping = {
        'doc': 'DOC',
        'titre': 'Titre',
        'chapitre': 'Chapitre',
        'section': 'Section',
        'article': 'Article',
        'contenu': 'Contenu',
        'texte': 'Contenu',
        'pages': 'Pages'
    }
    
    df = df.rename(columns={col: col_mapping.get(col, col) for col in df.columns})
    
    # Ajouter les colonnes manquantes
    for col in ['DOC', 'Titre', 'Chapitre', 'Section', 'Article', 'Contenu', 'Pages']:
        if col not in df.columns:
            df[col] = ''
    
    # Cr√©er le texte complet
    df['texte_complet'] = (
        df['Article'].fillna('').astype(str) + ' ' + 
        df['Contenu'].fillna('').astype(str)
    )
    df['texte_complet'] = df['texte_complet'].str.replace('\r\n', ' ').str.strip()
    
    return df

def prepare_chunks(df, source_file):
    """Pr√©pare les chunks pour l'indexation"""
    texts = []
    metadatas = []
    ids = []
    
    for idx, row in df.iterrows():
        text = str(row.get('texte_complet', '')).strip()
        
        # Ignorer les textes vides ou trop courts
        if not text or len(text) < 10:
            continue
        
        texts.append(text)
        
        # M√©tadonn√©es
        metadatas.append({
            'DOC': str(row.get('DOC', '')),
            'Titre': str(row.get('Titre', '')),
            'Chapitre': str(row.get('Chapitre', '')),
            'Section': str(row.get('Section', '')),
            'Article': str(row.get('Article', '')),
            'Pages': str(row.get('Pages', '')),
            'source': source_file
        })
        
        # ID unique
        file_name = os.path.basename(source_file).replace('.csv', '')
        ids.append(f"{file_name}_{idx}_{hash(text) % 1000000}")
    
    return texts, metadatas, ids

def index_texts(texts, metadatas, ids):
    """Indexe les textes dans ChromaDB"""
    if len(texts) == 0:
        print("‚ùå Aucun texte √† indexer")
        return
    
    client, collection = init_chroma()
    
    print(f"\nüìä Documents existants : {collection.count()}")
    
    # Indexer par batch pour √©viter les erreurs de m√©moire
    batch_size = 100
    total_indexed = 0
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        batch_metas = metadatas[i:i+batch_size]
        batch_ids = ids[i:i+batch_size]
        
        try:
            collection.add(
                documents=batch_texts,
                metadatas=batch_metas,
                ids=batch_ids
            )
            total_indexed += len(batch_texts)
            print(f"‚úÖ Index√© {total_indexed}/{len(texts)} documents")
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur batch {i}-{i+batch_size}: {e}")
            continue
    
    print(f"\nüéâ Total dans ChromaDB : {collection.count()} documents")

# ==================== SCRIPT PRINCIPAL ====================

if __name__ == "__main__":
    print("üöÄ D√©but de l'indexation...\n")
    
    csv_files = [
        "data/loi_articles.csv",
        "data/IGOC_2024_articles_corrigee2.csv",
        "data/Code_General_Des_Impots_Articles.csv"
    ]
    
    all_texts, all_metadatas, all_ids = [], [], []
    
    for csv_file in csv_files:
        if not os.path.exists(csv_file):
            print(f"‚ö†Ô∏è  Fichier introuvable : {csv_file}")
            continue
        
        print(f"üìÇ Traitement de {csv_file}...")
        
        try:
            # Charger et pr√©traiter
            df = load_csv(csv_file)
            df = preprocess_df(df)
            
            print(f"   üìÑ Lignes : {len(df)}")
            print(f"   üìã Colonnes : {list(df.columns)}")
            
            # Pr√©parer les chunks
            texts, metadatas, ids = prepare_chunks(df, csv_file)
            
            print(f"   ‚úÖ Chunks cr√©√©s : {len(texts)}")
            
            if len(texts) > 0:
                print(f"   üìù Exemple : {texts[0][:100]}...")
            
            all_texts.extend(texts)
            all_metadatas.extend(metadatas)
            all_ids.extend(ids)
            
        except Exception as e:
            print(f"   ‚ùå Erreur : {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # Indexation finale
    if all_texts:
        print(f"\n{'='*60}")
        print(f"üìä Total √† indexer : {len(all_texts)} documents")
        print(f"{'='*60}\n")
        index_texts(all_texts, all_metadatas, all_ids)
        print("\n‚úÖ Indexation termin√©e !")
    else:
        print("\n‚ùå Aucune donn√©e √† indexer ! V√©rifiez vos fichiers CSV.")