import requests
import json

# =======================
# ğŸ  Configuration LLM Local
# =======================
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_MODEL = "llama3.2"  # âœ… Llama 3.2

# =======================
# ğŸ’¬ Fonctions principales
# =======================
def ask_general(question: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse gÃ©nÃ©rale (non juridique) avec Llama 3.2 local.
    """
    print("\nğŸŒ Mode assistant gÃ©nÃ©ral activÃ© (Llama 3.2)")
    
    ai_response = call_ollama_general(question)
    
    if ai_response:
        return f"ğŸ’¬ **RÃ©ponse :**\n\n{ai_response.strip()}"
    
    return """ğŸ’¬ **RÃ©ponse :**

Bonjour ! Je suis un assistant conversationnel.

âš ï¸ Le modÃ¨le local n'est pas disponible. Assurez-vous qu'Ollama est dÃ©marrÃ©.

ğŸ’¡ **Pour dÃ©marrer Ollama :**
```bash
ollama serve
```

ğŸ’¡ **Je peux vous aider avec :**
- Questions sur le droit marocain
- Explications juridiques
- InterprÃ©tation d'articles de loi

N'hÃ©sitez pas Ã  me poser une question juridique ! âš–ï¸"""


def ask_juridique(question: str, context: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse juridique basÃ©e sur le contexte avec Llama 3.2 local.
    """
    if not context or not context.strip():
        return "âŒ Aucun article pertinent n'a Ã©tÃ© trouvÃ© dans la base de donnÃ©es juridique."

    print("\nâš–ï¸ Mode juridique activÃ© (Llama 3.2)")
    
    ai_response = call_ollama_juridique(question, context)

    if ai_response:
        return format_ai_response_with_sources(ai_response, context)

    print("âš ï¸ LLM local n'a pas rÃ©pondu â†’ Utilisation du fallback")
    return generate_smart_fallback(question, context)


# =======================
# ğŸ¦™ Ollama - Appels API
# =======================
def call_ollama_general(question: str) -> str:
    """
    Appelle Ollama (Llama 3.2) localement pour une question gÃ©nÃ©rale.
    """
    try:
        print(f"ğŸ”„ Appel Ã  Ollama (modÃ¨le: {OLLAMA_MODEL})...")
        
        prompt = f"""Tu es un assistant conversationnel utile et amical. 
RÃ©ponds en franÃ§ais de maniÃ¨re claire et concise.

Question : {question}

RÃ©ponse :"""

        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 300,
                    "top_p": 0.9
                }
            },
            timeout=180
        )

        if response.status_code == 200:
            data = response.json()
            answer = data.get("response", "").strip()
            print(f"âœ… RÃ©ponse reÃ§ue ({len(answer)} caractÃ¨res)")
            return answer
        else:
            print(f"âŒ Erreur Ollama : Status {response.status_code}")
            return None

    except requests.exceptions.Timeout:
        print("âŒ Timeout Ollama (180s dÃ©passÃ©)")
        return None
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama n'est pas dÃ©marrÃ©. Lancez : ollama serve")
        return None
    except Exception as e:
        print(f"âŒ Erreur Ollama : {str(e)[:100]}")
        return None


def call_ollama_juridique(question: str, context: str) -> str:
    """
    Appelle Ollama (Llama 3.2) localement pour une question juridique.
    """
    try:
        print(f"ğŸ”„ Appel juridique Ã  Ollama (modÃ¨le: {OLLAMA_MODEL})...")
        
        # Prompt plus structurÃ© et explicite
        prompt = f"""Tu es un assistant juridique expert en droit marocain. Tu dois expliquer les lois de maniÃ¨re claire et accessible.

CONTEXTE JURIDIQUE :
{context}

QUESTION DE L'UTILISATEUR :
{question}

INSTRUCTIONS IMPORTANTES :
1. Commence par une phrase d'introduction claire qui rÃ©pond directement Ã  la question
2. Explique les principes juridiques en langage simple (comme si tu parlais Ã  quelqu'un qui n'est pas juriste)
3. Base-toi UNIQUEMENT sur le contexte juridique fourni ci-dessus
4. Structure ta rÃ©ponse en 2-3 paragraphes maximum (150-250 mots)
5. Ne cite PAS les numÃ©ros d'articles (ils seront ajoutÃ©s automatiquement aprÃ¨s)
6. Termine par un conseil pratique ou une recommandation

RÃ‰PONSE EN FRANÃ‡AIS :"""

        response = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": OLLAMA_MODEL,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 400,
                    "top_p": 0.9,
                    "stop": ["\n\nQUESTION", "\n\nCONTEXTE"]
                }
            },
            timeout=180
        )

        if response.status_code == 200:
            data = response.json()
            answer = data.get("response", "").strip()
            
            # Nettoyer la rÃ©ponse
            answer = answer.replace("RÃ‰PONSE EN FRANÃ‡AIS:", "").strip()
            answer = answer.replace("RÃ©ponse :", "").strip()
            
            print(f"âœ… RÃ©ponse juridique reÃ§ue ({len(answer)} caractÃ¨res)")
            return answer
        else:
            print(f"âŒ Erreur Ollama : Status {response.status_code}")
            return None

    except requests.exceptions.Timeout:
        print("âŒ Timeout Ollama (180s dÃ©passÃ©)")
        return None
    except requests.exceptions.ConnectionError:
        print("âŒ Ollama n'est pas dÃ©marrÃ©. Lancez : ollama serve")
        return None
    except Exception as e:
        print(f"âŒ Erreur Ollama : {str(e)[:100]}")
        return None


# =======================
# ğŸ§¾ Mise en forme finale
# =======================
def format_ai_response_with_sources(ai_response: str, context: str) -> str:
    """
    Formate la rÃ©ponse IA + ajoute les articles Ã  la fin
    """
    articles = []
    for line in context.split('\n\n'):
        line = line.strip()
        if line and len(line) > 20:
            articles.append(line)

    formatted = f"""ğŸ’¬ **RÃ©ponse juridique :**

{ai_response.strip()}

---

ğŸ“š **RÃ©fÃ©rences lÃ©gales :**
"""

    for i, article in enumerate(articles[:3], 1):
        lines = article.split('\n')
        if len(lines) >= 2:
            reference = lines[0].strip()
            content = ' '.join(lines[1:]).strip()
            if len(content) > 250:
                content = content[:250] + "..."
            formatted += f"\n**{i}. {reference}**\n> {content}\n"

    formatted += "\n---\n_ğŸ’¼ Source : Base de donnÃ©es juridique marocaine "
    return formatted


# =======================
# ğŸ§© Fallback sans IA
# =======================
def generate_smart_fallback(question: str, context: str) -> str:
    """
    GÃ©nÃ¨re une rÃ©ponse basique si Ollama ne rÃ©pond pas
    """
    articles = []
    for line in context.split('\n\n'):
        line = line.strip()
        if line and len(line) > 20:
            articles.append(line)

    intro = f"ğŸ’¬ **RÃ©ponse juridique :**\n\n"
    intro += f"D'aprÃ¨s la lÃ©gislation marocaine, concernant votre question sur **{question}**, voici les dispositions pertinentes :\n\n"

    for i, article in enumerate(articles[:3], 1):
        lines = article.split('\n')
        if len(lines) >= 2:
            reference = lines[0].strip()
            content = ' '.join(lines[1:]).strip()
            if len(content) > 250:
                content = content[:250] + "..."
            intro += f"**{i}. {reference}**\n> {content}\n\n"

    intro += "---\nğŸ“Œ **Remarque :** Cette rÃ©ponse est basÃ©e sur les textes juridiques marocains en vigueur. Pour une interprÃ©tation dÃ©taillÃ©e, consultez un avocat.\n\n_ğŸ’¼ Source : Base de donnÃ©es juridique marocaine_"
    return intro