import requests
from config import HF_TOKEN

# =======================
# üí¨ Fonctions principales
# =======================
def ask_general(question: str) -> str:
    """
    G√©n√®re une r√©ponse g√©n√©rale (non juridique) avec l'IA.
    """
    print("\nüåê Mode assistant g√©n√©ral activ√©")
    
    # Essayer les mod√®les IA dans l'ordre
    ai_response = try_ai_models_general(question)
    
    if ai_response:
        return f"üí¨ **R√©ponse :**\n\n{ai_response.strip()}"
    
    # Fallback si aucun mod√®le ne r√©pond
    return """üí¨ **R√©ponse :**

Bonjour ! Je suis un assistant conversationnel.

Malheureusement, je ne peux pas r√©pondre √† votre question pour le moment car les services d'IA sont temporairement indisponibles.

üí° **Je peux vous aider avec :**
- Questions sur le droit marocain (codes p√©nal, civil, travail, etc.)
- Explications juridiques
- Interpr√©tation d'articles de loi

N'h√©sitez pas √† me poser une question juridique ! ‚öñÔ∏è"""


def try_ai_models_general(question: str) -> str:
    """Essaie les mod√®les IA pour une question g√©n√©rale (sans contexte juridique)"""
    
    # 1Ô∏è‚É£ OpenAI
    print("\nüîÑ Tentative 1/3 : OpenAI...")
    try:
        response = try_openai_general(question)
        if response:
            print("‚úÖ OpenAI a r√©pondu")
            return response
    except Exception as e:
        print(f"‚ùå OpenAI : {str(e)[:80]}")
    
    # 2Ô∏è‚É£ Groq
    print("\nüîÑ Tentative 2/3 : Groq...")
    try:
        response = try_groq_general(question)
        if response:
            print("‚úÖ Groq a r√©pondu")
            return response
    except Exception as e:
        print(f"‚ùå Groq : {str(e)[:80]}")
    
    # 3Ô∏è‚É£ HuggingFace
    print("\nüîÑ Tentative 3/3 : HuggingFace...")
    try:
        response = try_huggingface_general(question)
        if response:
            print("‚úÖ HuggingFace a r√©pondu")
            return response
    except Exception as e:
        print(f"‚ùå HuggingFace : {str(e)[:80]}")
    
    return None


def try_openai_general(question: str) -> str:
    try:
        from config import OPENAI_API_KEY
        from openai import OpenAI
        
        if not OPENAI_API_KEY or OPENAI_API_KEY == "sk-votre-cl√©-ici":
            return None
        
        client = OpenAI(api_key=OPENAI_API_KEY)
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un assistant conversationnel utile et amical. R√©ponds en fran√ßais de mani√®re claire et concise."},
                {"role": "user", "content": question}
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
    except:
        return None


def try_groq_general(question: str) -> str:
    try:
        from config import GROQ_API_KEY
        from groq import Groq
        
        if not GROQ_API_KEY or GROQ_API_KEY == "sk-votre-cl√©-ici":
            return None
        
        client = Groq(api_key=GROQ_API_KEY)
        
        completion = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "Tu es un assistant conversationnel utile et amical. R√©ponds en fran√ßais de mani√®re claire et concise."},
                {"role": "user", "content": question}
            ],
            max_tokens=300,
            temperature=0.7
        )
        
        return completion.choices[0].message.content.strip()
    except:
        return None


def try_huggingface_general(question: str) -> str:
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json"
    }
    
    models = [
        "mistralai/Mistral-7B-Instruct-v0.2",
        "meta-llama/Llama-2-7b-chat-hf"
    ]
    
    prompt = f"<s>[INST] {question} [/INST]"
    
    for model_id in models:
        try:
            response = requests.post(
                f"https://api-inference.huggingface.co/models/{model_id}",
                headers=headers,
                json={
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 250,
                        "temperature": 0.7,
                        "return_full_text": False
                    }
                },
                timeout=20
            )
            
            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 0 and 'generated_text' in data[0]:
                    text = data[0]['generated_text'].strip()
                    if text and len(text) > 30:
                        return text
        except:
            continue
    
    return None


def ask_juridique(question: str, context: str) -> str:
    """
    G√©n√®re une r√©ponse juridique bas√©e sur le contexte.
    """
    if not context or not context.strip():
        return "‚ùå Aucun article pertinent n'a √©t√© trouv√© dans la base de donn√©es juridique."

    # Essayer les mod√®les IA (OpenAI, Groq, HuggingFace)
    ai_response = try_ai_models(question, context)

    if ai_response:
        return format_ai_response_with_sources(ai_response, context)

    # Si aucun mod√®le n'a r√©pondu ‚Üí fallback structur√©
    print("‚ö†Ô∏è Tous les mod√®les IA ont √©chou√© ‚Üí Utilisation du fallback")
    return generate_smart_fallback(question, context)


# =======================
# üîÅ Gestion des mod√®les IA
# =======================
def try_ai_models(question: str, context: str) -> str:
    """Essaie diff√©rents mod√®les IA dans l'ordre"""
    
    # 1Ô∏è‚É£ OpenAI
    print("\nüîÑ Tentative 1/3 : OpenAI (GPT-3.5-turbo)...")
    try:
        openai_response = try_openai(question, context)
        if openai_response:
            print("‚úÖ OpenAI a r√©pondu avec succ√®s")
            return openai_response
        else:
            print("‚ùå OpenAI : Cl√© API invalide ou non configur√©e")
    except Exception as e:
        print(f"‚ùå OpenAI : Erreur - {str(e)[:100]}")

    # 2Ô∏è‚É£ Groq
    print("\nüîÑ Tentative 2/3 : Groq (Llama-3.1-8b)...")
    try:
        groq_response = try_groq(question, context)
        if groq_response:
            print("‚úÖ Groq a r√©pondu avec succ√®s")
            return groq_response
        else:
            print("‚ùå Groq : Cl√© API invalide ou non configur√©e")
    except Exception as e:
        print(f"‚ùå Groq : Erreur - {str(e)[:100]}")

    # 3Ô∏è‚É£ HuggingFace
    print("\nüîÑ Tentative 3/3 : HuggingFace (Mistral/Llama)...")
    try:
        hf_response = try_huggingface(question, context)
        if hf_response:
            print("‚úÖ HuggingFace a r√©pondu avec succ√®s")
            return hf_response
        else:
            print("‚ùå HuggingFace : Aucune r√©ponse valide obtenue")
    except Exception as e:
        print(f"‚ùå HuggingFace : Erreur - {str(e)[:100]}")

    print("\n‚ùå Aucun mod√®le IA n'a pu g√©n√©rer de r√©ponse")
    return None


# =======================
# üß† OpenAI
# =======================
def try_openai(question: str, context: str) -> str:
    try:
        from config import OPENAI_API_KEY
        from openai import OpenAI

        if not OPENAI_API_KEY or OPENAI_API_KEY == "sk-votre-cl√©-ici":
            print("   ‚è≠Ô∏è  Cl√© OpenAI non configur√©e (placeholder d√©tect√©)")
            return None

        client = OpenAI(api_key=OPENAI_API_KEY)

        prompt = f"""Tu es un assistant juridique marocain expert, capable d'expliquer clairement les lois et leurs implications.

Contexte :
{context}

Question :
{question}

R√©ponds en fran√ßais clair et naturel comme un avocat qui conseille un client :
- Donne une explication simple et structur√©e (150 √† 250 mots)
- Mentionne les principes g√©n√©raux du droit marocain
- Ne cite pas encore les articles (ils seront ajout√©s apr√®s)
- Termine par une phrase de conseil pratique

R√©ponse :
"""

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Tu es un assistant juridique marocain expert."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0.7
        )

        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur OpenAI : {str(e)[:80]}")
        return None


# =======================
# ‚ö° Groq
# =======================
def try_groq(question: str, context: str) -> str:
    try:
        from config import GROQ_API_KEY
        from groq import Groq

        if not GROQ_API_KEY or GROQ_API_KEY == "sk-votre-cl√©-ici":
            print("   ‚è≠Ô∏è  Cl√© Groq non configur√©e (placeholder d√©tect√©)")
            return None

        print(f"   üîë Cl√© Groq d√©tect√©e : {GROQ_API_KEY[:20]}...")
        client = Groq(api_key=GROQ_API_KEY)

        prompt = f"""Tu es un assistant juridique marocain expert, capable d'expliquer clairement les lois et leurs implications.

Contexte :
{context}

Question :
{question}

R√©ponds en fran√ßais clair et naturel comme un avocat qui conseille un client :
- Donne une explication simple et structur√©e (150 √† 250 mots)
- Mentionne les principes g√©n√©raux du droit marocain
- Ne cite pas encore les articles (ils seront ajout√©s apr√®s)
- Termine par une phrase de conseil pratique
"""

        completion = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[
                {"role": "system", "content": "Tu es un assistant juridique marocain expert."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=400,
            temperature=0.7
        )

        result = completion.choices[0].message.content.strip()
        print(f"   üìù R√©ponse Groq re√ßue : {len(result)} caract√®res")
        return result
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Erreur Groq : {str(e)[:80]}")
        return None


# =======================
# ü§ñ HuggingFace
# =======================
def try_huggingface(question: str, context: str) -> str:
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json"
    }

    models = [
        "mistralai/Mistral-7B-Instruct-v0.2",
        "meta-llama/Llama-2-7b-chat-hf"
    ]

    prompt = f"""<s>[INST] Tu es un assistant juridique marocain expert. R√©ponds de mani√®re claire, naturelle et structur√©e.

Contexte :
{context}

Question :
{question}

R√©ponds en fran√ßais clair et professionnel, comme un avocat qui conseille un client. Ne cite pas encore les articles (ils seront ajout√©s apr√®s). [/INST]
"""

    for i, model_id in enumerate(models, 1):
        try:
            print(f"   ü§ñ Test mod√®le {i}/{len(models)} : {model_id.split('/')[-1]}")
            
            response = requests.post(
                f"https://api-inference.huggingface.co/models/{model_id}",
                headers=headers,
                json={
                    "inputs": prompt,
                    "parameters": {
                        "max_new_tokens": 300,
                        "temperature": 0.7,
                        "return_full_text": False,
                        "do_sample": True
                    }
                },
                timeout=25
            )

            if response.status_code == 200:
                data = response.json()
                if isinstance(data, list) and len(data) > 0 and 'generated_text' in data[0]:
                    text = data[0]['generated_text'].strip()
                    if text and len(text) > 50:
                        print(f"   ‚úÖ Mod√®le {model_id.split('/')[-1]} a r√©pondu : {len(text)} caract√®res")
                        return text
                    else:
                        print(f"   ‚ö†Ô∏è  R√©ponse trop courte ({len(text)} caract√®res)")
                else:
                    print(f"   ‚ö†Ô∏è  Format de r√©ponse invalide")
            elif response.status_code == 503:
                print(f"   ‚è≥ Mod√®le en cours de chargement (503)")
            else:
                print(f"   ‚ùå Erreur HTTP {response.status_code}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Erreur : {str(e)[:60]}")
            continue

    return None


# =======================
# üßæ Mise en forme finale
# =======================
def format_ai_response_with_sources(ai_response: str, context: str) -> str:
    """
    Formate la r√©ponse IA comme ChatGPT + ajoute les articles √† la fin
    """
    # Extraire les articles du contexte
    articles = []
    for line in context.split('\n\n'):
        line = line.strip()
        if line and len(line) > 20:
            articles.append(line)

    formatted = f"""üí¨ **R√©ponse juridique :**

{ai_response.strip()}

---

üìö **R√©f√©rences l√©gales :**
"""

    # Ajouter jusqu'√† 3 articles
    for i, article in enumerate(articles[:3], 1):
        lines = article.split('\n')
        if len(lines) >= 2:
            reference = lines[0].strip()
            content = ' '.join(lines[1:]).strip()
            if len(content) > 250:
                content = content[:250] + "..."
            formatted += f"\n**{i}. {reference}**\n> {content}\n"

    formatted += "\n---\n_üíº Source : Base de donn√©es juridique marocaine_"
    return formatted


# =======================
# üß© Fallback sans IA
# =======================
def generate_smart_fallback(question: str, context: str) -> str:
    articles = []
    for line in context.split('\n\n'):
        line = line.strip()
        if line and len(line) > 20:
            articles.append(line)

    intro = f"üí¨ **R√©ponse juridique :**\n\n"
    intro += f"D'apr√®s la l√©gislation marocaine, concernant votre question sur **{question}**, voici les dispositions pertinentes :\n\n"

    for i, article in enumerate(articles[:3], 1):
        lines = article.split('\n')
        if len(lines) >= 2:
            reference = lines[0].strip()
            content = ' '.join(lines[1:]).strip()
            if len(content) > 250:
                content = content[:250] + "..."
            intro += f"**{i}. {reference}**\n> {content}\n\n"

    intro += "---\nüìå **Remarque :** Cette r√©ponse est bas√©e sur les textes juridiques marocains en vigueur. Pour une interpr√©tation d√©taill√©e de votre situation, il est conseill√© de consulter un avocat.\n\n_üíº Source : Base de donn√©es juridique marocaine_"
    return intro